{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\" \n",
    "n_episodes = 1000\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "model = mp.Initialize.model_definition.NNmodel().dense_multi()\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "replay_memory = deque(maxlen = p.buffer_size)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "reward_list = [ ]\n",
    "reward_inf_list = [ ]\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"episode_step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == 1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer: \n",
    "                \"\"\" (replay_memory, info, step_ended, \n",
    "                [수직방향] layer 몇 개씩 묶어서 penalty 부여할지, [수평방향] 좌/우 몇개씩 penalty 부여할지, layer 묶음 당 penalty 어떻게 줄지(총 4단계)]) \n",
    "                \"\"\"\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 1, 2, [-50, -30, -10, 0])\n",
    "                # 아래 코드로 replay penalty가 잘 먹히는지 확인 가능\n",
    "                # print(replay_memory[-1][2]) \n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        \"한번에 평균내서 weight update\"\n",
    "        # mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory, episode)\n",
    "        \n",
    "        \"각 점마다 weight update\"\n",
    "        mp.Train.model_training.training_step_each_DDQN(model, model_target, replay_memory, episode) \n",
    "        \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_train_plot(reward_list, episode)\n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_inf_plot(reward_inf_list, episode)\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\"\n",
    "start_episode = 100\n",
    "n_episodes = 650\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "model = tf.keras.models.load_model(f'model_storage/DDQN_{p.mesh_name}_episode_{start_episode}')\n",
    "\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "with open(f'replay_memory/replay_memory_{start_episode}.p', 'rb') as fr:  \n",
    "    replay_memory = pickle.load(fr)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "with open(f'Episode_reward_train/reward_epi_{start_episode}.p', 'rb') as fe:     \n",
    "    reward_list = pickle.load(fe)\n",
    "##\n",
    "with open(f'Episode_reward_inf/reward_epi_{start_episode}.p', 'rb') as fei:     \n",
    "    reward_inf_list = pickle.load(fei)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(start_episode+1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"Episode_Step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == start_episode+1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer:\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 3, 3)\n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        \"한번에 평균내서 weight update\"\n",
    "        # mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory, episode)\n",
    "        \n",
    "        \"각 점마다 weight update\"\n",
    "        mp.Train.model_training.training_step_each_DDQN(model, model_target, replay_memory, episode)\n",
    "         \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        # target model에 weight 그대로 복사\n",
    "        # model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        # target model에 \"tau\"만큼만 weight 복사\n",
    "        mp.Train.target_update.soft_update(model_target.variables, model.variables, p.tau)\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_train_plot(reward_list, episode)\n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_inf_plot(reward_inf_list, episode)\n",
    "\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
