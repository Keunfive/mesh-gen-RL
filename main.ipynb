{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\" \n",
    "n_episodes = 1000\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "model = mp.Initialize.model_definition.NNmodel().dense_multi()\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "replay_memory = deque(maxlen = p.buffer_size)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "reward_list = [ ]\n",
    "reward_inf_list = [ ]\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"episode_step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == 1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer: \n",
    "                \"\"\" (replay_memory, info, step_ended, \n",
    "                [수직방향] layer 몇 개씩 묶어서 penalty 부여할지, [수평방향] 좌/우 몇개씩 penalty 부여할지, layer 묶음 당 penalty 어떻게 줄지(총 4단계)]) \n",
    "                \"\"\"\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 1, 2, [-50, -30, -10, 0])\n",
    "                # 아래 코드로 replay penalty가 잘 먹히는지 확인 가능\n",
    "                # print(replay_memory[-1][2]) \n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        \"한번에 평균내서 weight update\"\n",
    "        # mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory, episode)\n",
    "        \n",
    "        \"각 점마다 weight update\"\n",
    "        mp.Train.model_training.training_step_each_DDQN(model, model_target, replay_memory, episode) \n",
    "        \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_train_plot(reward_list, episode)\n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_inf_plot(reward_inf_list, episode)\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 30)           0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          3968        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          16512       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 25)           3225        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,217\n",
      "Trainable params: 40,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Assets written to: model_storage/DDQN_spline_1_1_episode_4050\\assets\n",
      "INFO:tensorflow:Assets written to: model_storage/DDQN_spline_1_1_episode_4100\\assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory, episode)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m각 점마다 weight update\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 104\u001b[0m     mp\u001b[39m.\u001b[39;49mTrain\u001b[39m.\u001b[39;49mmodel_training\u001b[39m.\u001b[39;49mtraining_step_each_DDQN(model, model_target, replay_memory, episode)\n\u001b[0;32m    106\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mepisode (episode_inference)회마다 Inference\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m (episode_inference) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\KEUNOH Lim\\vscodeprojects\\Mesh_v0704\\Meshpkg\\Train\\model_training.py:100\u001b[0m, in \u001b[0;36mtraining_step_each_DDQN\u001b[1;34m(model, model_target, replay_memory, episode)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mnext state normalization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m next_state_new \u001b[39m=\u001b[39m get_state\u001b[39m.\u001b[39mget_new_state_2(next_state)\n\u001b[1;32m--> 100\u001b[0m next_Q, next_action \u001b[39m=\u001b[39m get_next_action(model, model_target, next_state_new)\n\u001b[0;32m    101\u001b[0m next_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39mone_hot(next_action, p\u001b[39m.\u001b[39mn_actions), tf\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    102\u001b[0m max_next_Q \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(next_Q \u001b[39m*\u001b[39m next_mask, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\KEUNOH Lim\\vscodeprojects\\Mesh_v0704\\Meshpkg\\Env\\Action.py:124\u001b[0m, in \u001b[0;36mget_next_action\u001b[1;34m(model, model_target, next_state_new, num_iter)\u001b[0m\n\u001b[0;32m    116\u001b[0m     next_action_neighbor_list\u001b[39m.\u001b[39mappend( tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mmatmul(\n\u001b[0;32m    117\u001b[0m         tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mone_hot(next_action[i][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], p\u001b[39m.\u001b[39mn_actions), [p\u001b[39m.\u001b[39mn_actions, \u001b[39m1\u001b[39m]), \n\u001b[0;32m    118\u001b[0m         tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mone_hot(next_action[i][(j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m p\u001b[39m.\u001b[39msurf_length], p\u001b[39m.\u001b[39mn_actions), [\u001b[39m1\u001b[39m, p\u001b[39m.\u001b[39mn_actions])), [\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) )\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m p\u001b[39m.\u001b[39mact_shape \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m: \u001b[39m# [1, 20]\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     next_action_neighbor_list\u001b[39m.\u001b[39mappend( tf\u001b[39m.\u001b[39mconcat(\n\u001b[0;32m    122\u001b[0m         [tf\u001b[39m.\u001b[39mconcat([ tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mone_hot(next_action[i][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), [\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\n\u001b[0;32m    123\u001b[0m                     tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mone_hot(next_action[i][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), [\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) ], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[1;32m--> 124\u001b[0m         tf\u001b[39m.\u001b[39mconcat([ tf\u001b[39m.\u001b[39;49mreshape(tf\u001b[39m.\u001b[39;49mone_hot(next_action[i][(j \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m%\u001b[39;49m p\u001b[39m.\u001b[39;49msurf_length] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m), [\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]),\n\u001b[0;32m    125\u001b[0m                     tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mone_hot(next_action[i][(j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m p\u001b[39m.\u001b[39msurf_length] \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), [\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) ], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) )\n\u001b[0;32m    127\u001b[0m \u001b[39m###############################################\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1097\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1098\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:197\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmanip.reshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     62\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(tensor, shape, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[39m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[0;32m    198\u001b[0m   tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    199\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8540\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8538\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   8539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 8540\u001b[0m   \u001b[39mreturn\u001b[39;00m reshape_eager_fallback(\n\u001b[0;32m   8541\u001b[0m       tensor, shape, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[0;32m   8542\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[0;32m   8543\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8565\u001b[0m, in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   8563\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [tensor, shape]\n\u001b[0;32m   8564\u001b[0m _attrs \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, _attr_T, \u001b[39m\"\u001b[39m\u001b[39mTshape\u001b[39m\u001b[39m\"\u001b[39m, _attr_Tshape)\n\u001b[1;32m-> 8565\u001b[0m _result \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49mexecute(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mReshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m_inputs_flat, attrs\u001b[39m=\u001b[39;49m_attrs,\n\u001b[0;32m   8566\u001b[0m                            ctx\u001b[39m=\u001b[39;49mctx, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   8567\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n\u001b[0;32m   8568\u001b[0m   _execute\u001b[39m.\u001b[39mrecord_gradient(\n\u001b[0;32m   8569\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReshape\u001b[39m\u001b[39m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\startgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\"\n",
    "start_episode = 4000\n",
    "n_episodes = 6000\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "model = tf.keras.models.load_model(f'model_storage/DDQN_{p.mesh_name}_episode_{start_episode}')\n",
    "\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "with open(f'replay_memory/replay_memory_{start_episode}.p', 'rb') as fr:  \n",
    "    replay_memory = pickle.load(fr)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "with open(f'Episode_reward_train/reward_epi_{start_episode}.p', 'rb') as fe:     \n",
    "    reward_list = pickle.load(fe)\n",
    "##\n",
    "with open(f'Episode_reward_inf/reward_epi_{start_episode}.p', 'rb') as fei:     \n",
    "    reward_inf_list = pickle.load(fei)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(start_episode+1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"episode_step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == start_episode+1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer:\n",
    "                \"\"\" (replay_memory, info, step_ended, \n",
    "                [수직방향] layer 몇 개씩 묶어서 penalty 부여할지, [수평방향] 좌/우 몇개씩 penalty 부여할지, layer 묶음 당 penalty 어떻게 줄지(총 4단계)]) \n",
    "                \"\"\"\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 1, 2, [-50, -30, -10, 0])\n",
    "                # 아래 코드로 replay penalty가 잘 먹히는지 확인 가능\n",
    "                # print(replay_memory[-1][2]) \n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        \"한번에 평균내서 weight update\"\n",
    "        # mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory, episode)\n",
    "        \n",
    "        \"각 점마다 weight update\"\n",
    "        mp.Train.model_training.training_step_each_DDQN(model, model_target, replay_memory, episode)\n",
    "         \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        # target model에 weight 그대로 복사\n",
    "        # model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        # target model에 \"tau\"만큼만 weight 복사\n",
    "        mp.Train.target_update.soft_update(model_target.variables, model.variables, p.tau)\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_train_plot(reward_list, episode)\n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_inf_plot(reward_inf_list, episode)\n",
    "\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
